{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# PyTorch 기본 구성 요소 (Core Components)\n",
    "\n",
    "이 노트북은 PyTorch의 핵심 개념 학습을 위한 코드 예제를 포함합니다.\n",
    "예: 텐서, 자동 미분, 모듈 기본, 활성화 함수 등\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np # NumPy bridge 예제를 위해 import\n",
    "import torch.nn as nn # nn.Module, nn.Sequential 등을 위해 import\n",
    "import torch.nn.functional as F # 활성화 함수, 손실 함수 등을 위해 import\n",
    "import torch.optim as optim # 옵티마이저를 위해 import\n",
    "from torch.optim.lr_scheduler import StepLR # 학습률 스케줄러를 위해 import\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. 텐서 (Tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로부터 직접 텐서 생성\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Tensor from data:\\n {x_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy 배열로부터 텐서 생성\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"Tensor from NumPy array:\\n {x_np}\")\n",
    "\n",
    "# NumPy 배열 변경 시 텐서도 변경 (메모리 공유)\n",
    "np_array[0, 0] = 10\n",
    "print(f\"NumPy array after change:\\n {np_array}\")\n",
    "print(f\"Tensor from NumPy array after change:\\n {x_np}\") # x_np도 변경됨\n",
    "x_np[0, 0] = 1 # 텐서 변경 시 NumPy 배열도 변경\n",
    "print(f\"NumPy array after tensor change:\\n {np_array}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 텐서를 NumPy 배열로 변환\n",
    "tensor_to_np = torch.ones(5)\n",
    "numpy_from_tensor = tensor_to_np.numpy()\n",
    "print(f\"NumPy array from tensor:\\n {numpy_from_tensor}\")\n",
    "tensor_to_np.add_(1) # 텐서 변경\n",
    "print(f\"NumPy array after original tensor changed:\\n {numpy_from_tensor}\") # NumPy 배열도 변경됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 텐서로부터 텐서 생성 (구조는 유지, 값은 랜덤)\n",
    "x_ones = torch.ones_like(x_data) # x_data의 속성을 유지합니다.\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "x_zeros = torch.zeros_like(x_data)\n",
    "print(f\"Zeros Tensor: \\n {x_zeros} \\n\")\n",
    "x_rand_like = torch.rand_like(x_data, dtype=torch.float) # x_data의 자료형을 float으로 덮어씁니다.\n",
    "print(f\"Random Tensor (like x_data): \\n {x_rand_like} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모양(shape)을 사용한 텐서 생성\n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "randn_tensor = torch.randn(shape) # 정규 분포\n",
    "empty_tensor = torch.empty(shape) # 초기화되지 않은 값\n",
    "arange_tensor = torch.arange(0, 10, 2) # 0부터 10 전까지 2 간격 (0, 2, 4, 6, 8)\n",
    "linspace_tensor = torch.linspace(0, 10, 5) # 0부터 10까지 5개 균등 간격\n",
    "\n",
    "print(f\"Random Tensor with shape {shape}: \\n {rand_tensor} \\n\")\n",
    "print(f\"Randn Tensor with shape {shape}: \\n {randn_tensor} \\n\")\n",
    "print(f\"Empty Tensor with shape {shape}: \\n {empty_tensor} \\n\") # 실행 시마다 다른 값\n",
    "print(f\"Arange Tensor: {arange_tensor}\\n\")\n",
    "print(f\"Linspace Tensor: {linspace_tensor}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 속성\n",
    "tensor_attr = torch.rand(3,4)\n",
    "print(f\"Shape of tensor_attr: {tensor_attr.shape}\")\n",
    "print(f\"Size of tensor_attr: {tensor_attr.size()}\") # shape과 동일\n",
    "print(f\"Datatype of tensor_attr: {tensor_attr.dtype}\")\n",
    "print(f\"Device tensor_attr is stored on: {tensor_attr.device}\")\n",
    "print(f\"Number of dimensions of tensor_attr (ndim): {tensor_attr.ndim}\")\n",
    "print(f\"Number of dimensions of tensor_attr (dim()): {tensor_attr.dim()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 연산 - 장치 이동\n",
    "# GPU가 존재하면 텐서를 이동합니다\n",
    "if torch.cuda.is_available():\n",
    "    tensor_op = tensor_attr.to('cuda')\n",
    "    print(f\"Device tensor_op is stored on after to('cuda'): {tensor_op.device}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    tensor_op = tensor_attr.to('mps')\n",
    "    print(f\"Device tensor_op is stored on after to('mps'): {tensor_op.device}\")\n",
    "else:\n",
    "    tensor_op = tensor_attr # CPU 그대로 사용\n",
    "    print(f\"Device tensor_op is stored on: {tensor_op.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy 스타일의 인덱싱과 슬라이싱\n",
    "indexing_tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {indexing_tensor[0]}\")\n",
    "print(f\"First column: {indexing_tensor[:, 0]}\")\n",
    "print(f\"Last column: {indexing_tensor[..., -1]}\")\n",
    "indexing_tensor[:,1] = 0\n",
    "print(f\"Tensor after modifying one column:\\n {indexing_tensor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 합치기\n",
    "t_cat = torch.cat([indexing_tensor, indexing_tensor, indexing_tensor], dim=1)\n",
    "print(f\"Concatenated tensor (dim=1):\\n {t_cat}\")\n",
    "t_stack = torch.stack([indexing_tensor, indexing_tensor, indexing_tensor], dim=0) # 새로운 차원(dim=0)으로 쌓음\n",
    "print(f\"Stacked tensor (dim=0):\\n {t_stack}, shape: {t_stack.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 산술 연산\n",
    "# 두 텐서 간의 행렬 곱(matrix multiplication)\n",
    "y1 = indexing_tensor @ indexing_tensor.T\n",
    "print(f\"Matrix multiplication (y1):\\n {y1}\")\n",
    "\n",
    "# 요소별 곱(element-wise product)\n",
    "z1 = indexing_tensor * indexing_tensor\n",
    "print(f\"Element-wise product (z1):\\n {z1}\")\n",
    "\n",
    "# 단일-요소 텐서\n",
    "agg = indexing_tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(f\"Sum of tensor: {agg}, Type: {type(agg)}\")\n",
    "print(f\"Value of sum (item()): {agg_item}, Type: {type(agg_item)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바꿔치기(in-place) 연산\n",
    "# 연산 결과를 피연산자에 저장하는 연산 (예: x.copy_(y), x.t_())은 _ 접미사를 갖습니다.\n",
    "inplace_tensor = torch.ones(2,2)\n",
    "print(f\"Original tensor for inplace op:\\n {inplace_tensor} \\n\")\n",
    "inplace_tensor.add_(5)\n",
    "print(f\"Tensor after add_(5):\\n {inplace_tensor}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. 자동 미분 (torch.autograd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ag = torch.ones(5)  # input tensor\n",
    "y_ag = torch.zeros(3)  # expected output\n",
    "w_ag = torch.randn(5, 3, requires_grad=True)\n",
    "b_ag = torch.randn(3, requires_grad=True)\n",
    "z_ag = torch.matmul(x_ag, w_ag)+b_ag\n",
    "loss_ag = F.binary_cross_entropy_with_logits(z_ag, y_ag)\n",
    "\n",
    "print(f\"Gradient function for z_ag = {z_ag.grad_fn}\")\n",
    "print(f\"Gradient function for loss_ag = {loss_ag.grad_fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ag.backward() # 역전파 실행\n",
    "print(f\"Gradient for w_ag:\\n {w_ag.grad}\")\n",
    "print(f\"Gradient for b_ag:\\n {b_ag.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient 추적 멈추기\n",
    "# 1. torch.no_grad() 컨텍스트 매니저\n",
    "with torch.no_grad():\n",
    "    z_with_no_grad_block = torch.matmul(x_ag, w_ag)+b_ag\n",
    "print(f\"z_with_no_grad_block.requires_grad: {z_with_no_grad_block.requires_grad}\")\n",
    "\n",
    "# 2. .detach()\n",
    "z_det = z_ag.detach() # z_ag와 데이터는 공유하지만, 연산 기록은 분리\n",
    "print(f\"z_det.requires_grad: {z_det.requires_grad}\")\n",
    "\n",
    "# 3. requires_grad_() (in-place)\n",
    "temp_tensor = torch.randn(2,2, requires_grad=True)\n",
    "print(f\"temp_tensor.requires_grad before: {temp_tensor.requires_grad}\")\n",
    "temp_tensor.requires_grad_(False)\n",
    "print(f\"temp_tensor.requires_grad after: {temp_tensor.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. 모듈 및 클래스 기본 (torch.nn.Module 소개)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 nn.Module 예시 (다음 섹션 \"PyTorch 핵심 구현 패턴\"에서 더 자세히 다룸)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(10, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        x_output = self.linear1(x_input)\n",
    "        x_output = self.relu(x_output)\n",
    "        x_output = self.linear2(x_output)\n",
    "        return x_output\n",
    "\n",
    "model_example = SimpleModel()\n",
    "print(\"SimpleModel instance:\")\n",
    "print(model_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 확인\n",
    "for name, param in model_example.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter name: {name}, Shape: {param.shape}\")\n",
    "\n",
    "input_tensor_sm = torch.randn(1, 10)\n",
    "output_tensor_sm = model_example(input_tensor_sm)\n",
    "print(f\"Input tensor shape for SimpleModel: {input_tensor_sm.shape}\")\n",
    "print(f\"Output tensor shape for SimpleModel: {output_tensor_sm.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential 사용 예시\n",
    "seq_model = nn.Sequential(\n",
    "    nn.Linear(10, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 5),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(5, 1)\n",
    ")\n",
    "print(\"\\nnn.Sequential model instance:\")\n",
    "print(seq_model)\n",
    "output_tensor_seq = seq_model(input_tensor_sm)\n",
    "print(f\"Output tensor shape for Sequential model: {output_tensor_seq.shape}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. 활성화 함수 (Activation Functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_activation = torch.randn(2, 3) * 2 # 다양한 값을 가지도록 조정\n",
    "print(f\"Input for Activations:\\n {input_activation}\")\n",
    "\n",
    "relu_fn = nn.ReLU()\n",
    "output_activation_relu = relu_fn(input_activation)\n",
    "print(f\"Output of ReLU:\\n {output_activation_relu}\")\n",
    "\n",
    "sigmoid_fn = nn.Sigmoid()\n",
    "output_activation_sigmoid = sigmoid_fn(input_activation)\n",
    "print(f\"Output of Sigmoid:\\n {output_activation_sigmoid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_fn = nn.Tanh()\n",
    "output_activation_tanh = tanh_fn(input_activation)\n",
    "print(f\"Output of Tanh:\\n {output_activation_tanh}\")\n",
    "\n",
    "leaky_relu_fn = nn.LeakyReLU(negative_slope=0.1)\n",
    "output_activation_leaky = leaky_relu_fn(input_activation)\n",
    "print(f\"Output of LeakyReLU (negative_slope=0.1):\\n {output_activation_leaky}\")\n",
    "\n",
    "# Softmax는 보통 다중 클래스 분류의 출력에 사용 (합이 1이 되는 확률 분포)\n",
    "softmax_fn = nn.Softmax(dim=1) # dim=1은 각 행에 대해 softmax 적용\n",
    "input_softmax = torch.randn(2, 4) # (batch_size, num_classes)\n",
    "output_softmax = softmax_fn(input_softmax)\n",
    "print(f\"Input for Softmax:\\n {input_softmax}\")\n",
    "print(f\"Output of Softmax (dim=1):\\n {output_softmax}\")\n",
    "print(f\"Sum of Softmax output per row: {output_softmax.sum(dim=1)}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. 손실 함수 (Loss Functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시: MSE Loss (Mean Squared Error) - 회귀 문제\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "predicted_mse = torch.randn(5, requires_grad=True)\n",
    "target_mse = torch.randn(5)\n",
    "loss_val_mse = mse_loss_fn(predicted_mse, target_mse)\n",
    "print(f\"Predicted (MSE): {predicted_mse.detach()}\") # 출력용으로 detach\n",
    "print(f\"Target (MSE): {target_mse}\")\n",
    "print(f\"MSE Loss: {loss_val_mse.item()}\") # .item()으로 스칼라 값 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시: CrossEntropyLoss - 다중 클래스 분류 문제\n",
    "# (내부적으로 LogSoftmax와 NLLLoss를 결합)\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "predicted_logits_ce = torch.randn(3, 5, requires_grad=True) # (batch_size, num_classes) - raw logits\n",
    "target_labels_ce = torch.tensor([1, 0, 4]) # (batch_size) - class indices (0 ~ C-1)\n",
    "loss_val_ce = ce_loss_fn(predicted_logits_ce, target_labels_ce)\n",
    "print(f\"Predicted Logits (CE, shape {predicted_logits_ce.shape}): \\n {predicted_logits_ce.detach()}\")\n",
    "print(f\"Target Labels (CE, shape {target_labels_ce.shape}): {target_labels_ce}\")\n",
    "print(f\"CrossEntropy Loss: {loss_val_ce.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시: BCEWithLogitsLoss - 이진 분류 또는 다중 레이블 분류 문제\n",
    "# (Sigmoid + BCELoss 결합, 수치적 안정성)\n",
    "bce_logits_loss_fn = nn.BCEWithLogitsLoss()\n",
    "predicted_logits_bce = torch.randn(4, 1, requires_grad=True) # 4개 샘플, 이진 분류 (출력 로짓)\n",
    "target_bce = torch.tensor([[1.0], [0.0], [1.0], [0.0]]) # 각 샘플의 타겟 (0 또는 1)\n",
    "loss_val_bce_logits = bce_logits_loss_fn(predicted_logits_bce, target_bce)\n",
    "print(f\"Predicted Logits (BCEWithLogits): \\n {predicted_logits_bce.detach()}\")\n",
    "print(f\"Target (BCEWithLogits): \\n {target_bce}\")\n",
    "print(f\"BCEWithLogits Loss: {loss_val_bce_logits.item()}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. 옵티마이저 (torch.optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 모델과 더미 데이터 생성\n",
    "model_for_optim = nn.Linear(10, 1) # 입력 특성 10개, 출력 특성 1개\n",
    "dummy_input = torch.randn(5, 10) # 배치 크기 5, 입력 특성 10개\n",
    "dummy_target = torch.randn(5, 1)  # 배치 크기 5, 출력 특성 1개\n",
    "\n",
    "# 옵티마이저 정의 (예: SGD)\n",
    "optimizer_sgd = optim.SGD(model_for_optim.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 옵티마이저 정의 (예: Adam)\n",
    "optimizer_adam = optim.Adam(model_for_optim.parameters(), lr=0.001)\n",
    "\n",
    "# 학습률 스케줄러 예시 (StepLR)\n",
    "# 2 에폭마다 학습률에 gamma(0.1)를 곱함\n",
    "scheduler_steplr = StepLR(optimizer_adam, step_size=2, gamma=0.1)\n",
    "\n",
    "print(f\"Initial LR for Adam: {optimizer_adam.param_groups[0]['lr']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반적인 훈련 스텝 (간략화된 루프)\n",
    "loss_fn_optim = nn.MSELoss()\n",
    "for epoch in range(5): # 5 에폭만 예시로 실행\n",
    "    # 1. 옵티마이저의 기울기 초기화\n",
    "    optimizer_adam.zero_grad() # Adam 옵티마이저 사용 예시\n",
    "\n",
    "    # 2. 모델을 통해 예측 수행\n",
    "    predictions = model_for_optim(dummy_input)\n",
    "\n",
    "    # 3. 손실 계산\n",
    "    loss_optim = loss_fn_optim(predictions, dummy_target)\n",
    "\n",
    "    # 4. 역전파 수행\n",
    "    loss_optim.backward()\n",
    "\n",
    "    # 5. 옵티마이저 스텝 (파라미터 업데이트)\n",
    "    optimizer_adam.step()\n",
    "    \n",
    "    # 6. 스케줄러 스텝 (에폭 단위)\n",
    "    scheduler_steplr.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss_optim.item():.4f}, Current LR: {optimizer_adam.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "print(\"Optimizer step and LR scheduler example completed.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. 장치 할당 (.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "# 모델과 텐서를 해당 장치로 옮김\n",
    "tensor_to_move = torch.randn(3,3)\n",
    "print(f\"Tensor original device: {tensor_to_move.device}\")\n",
    "tensor_to_move_on_device = tensor_to_move.to(device) # 새로운 변수에 할당\n",
    "print(f\"Tensor new device: {tensor_to_move_on_device.device}\")\n",
    "\n",
    "model_to_move = SimpleModel() # 위에서 정의한 SimpleModel 사용\n",
    "print(f\"Model original device (any parameter): {next(model_to_move.parameters()).device}\")\n",
    "model_to_move.to(device) # 모델 자체를 이동 (in-place 효과)\n",
    "print(f\"Model new device (any parameter): {next(model_to_move.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 장치 간 데이터 이동 시 주의: 연산은 동일 장치에 있는 텐서들 간에만 가능\n",
    "try:\n",
    "    cpu_tensor = torch.randn(3,3, device='cpu')\n",
    "    # device가 cpu가 아닌 경우에만 gpu_tensor를 다른 장치로 만듦\n",
    "    if device != 'cpu':\n",
    "        gpu_tensor = torch.randn(3,3).to(device)\n",
    "        result = cpu_tensor + gpu_tensor # 오류 발생 예상\n",
    "        print(f\"Result of cpu + gpu tensor (should not happen): {result}\")\n",
    "    else:\n",
    "        print(\"Skipping cross-device operation test as device is CPU.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Expected error for cross-device operation: {e}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. 가중치 초기화 (Weight Initialization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module의 가중치는 기본적으로 특정 방식으로 초기화됨 (예: Linear, Conv2d는 Kaiming He 초기화 등)\n",
    "linear_layer_default_init = nn.Linear(5, 2)\n",
    "print(f\"Default initialized weights for Linear layer:\\n {linear_layer_default_init.weight.data}\") # .data로 값만 출력\n",
    "print(f\"Default initialized bias for Linear layer:\\n {linear_layer_default_init.bias.data}\")\n",
    "\n",
    "# 수동으로 가중치 초기화 예시\n",
    "linear_layer_manual_init = nn.Linear(5, 2)\n",
    "\n",
    "# Xavier Uniform 초기화\n",
    "nn.init.xavier_uniform_(linear_layer_manual_init.weight)\n",
    "# Bias는 0으로 초기화\n",
    "nn.init.zeros_(linear_layer_manual_init.bias)\n",
    "\n",
    "print(f\"Manually initialized weights (Xavier Uniform):\\n {linear_layer_manual_init.weight.data}\")\n",
    "print(f\"Manually initialized bias (Zeros):\\n {linear_layer_manual_init.bias.data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 전체에 적용하는 함수 예시\n",
    "class ModelWithCustomInit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1) # 예시 Conv2d 추가\n",
    "        self.bn1 = nn.BatchNorm2d(16) # 예시 BatchNorm2d 추가\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 128) # 입력 크기는 conv, bn 통과 후 flatten 가정 (예시 크기)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self._initialize_weights() # 생성자에서 호출\n",
    "\n",
    "    def forward(self, x_input): # (batch, 3, H, W) 형태의 입력 가정 (예: 3x8x8)\n",
    "        x_output = self.conv1(x_input)\n",
    "        x_output = self.bn1(x_output)\n",
    "        x_output = self.relu1(x_output)\n",
    "        x_output = x_output.view(x_output.size(0), -1) # Flatten\n",
    "        x_output = self.fc1(x_output)\n",
    "        x_output = self.fc2(x_output)\n",
    "        return x_output\n",
    "\n",
    "    @torch.no_grad() # 초기화 과정은 기울기 추적 불필요\n",
    "    def _initialize_weights(self):\n",
    "        print(\"Applying custom initialization...\")\n",
    "        for m in self.modules(): # self.modules()는 자기 자신 포함 모든 sub-module을 순회\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                print(f\"Initializing Conv2d: {m}\")\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                print(f\"Initializing BatchNorm2d: {m}\")\n",
    "                nn.init.constant_(m.weight, 1) # 보통 weight는 1\n",
    "                nn.init.constant_(m.bias, 0)   # bias는 0\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                print(f\"Initializing Linear: {m}\")\n",
    "                nn.init.xavier_normal_(m.weight) # Linear는 Xavier Normal 예시\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성 시 _initialize_weights 호출됨\n",
    "model_custom_init = ModelWithCustomInit()\n",
    "print(\"ModelWithCustomInit instantiated and weights initialized.\")\n",
    "# 예시로 fc1 레이어 가중치 일부 확인\n",
    "print(f\"Layer fc1 weights after custom init (first 5 values): {model_custom_init.fc1.weight.data.flatten()[:5]}\")\n",
    "print(f\"Layer conv1 weights after custom init (first 5 values of first filter): {model_custom_init.conv1.weight.data[0,0,0,:5].flatten()}\")\n",
    "\n",
    "print(\"\\nPyTorch Core Components 노트북 실행 완료.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
