{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# PyTorch 핵심 구현 패턴 (Key Implementation Patterns)\n",
        "\n",
        "이 노트북은 PyTorch에서 자주 사용되는 핵심 구현 패턴들을 다룹니다:\n",
        "\n",
        "1. **nn.Module 상속 패턴**\n",
        "2. **Dataset 커스텀 패턴**\n",
        "3. **DataLoader 사용법**\n",
        "4. **train()/eval() 모드와 no_grad() 패턴**\n",
        "5. **모델 저장/로드 패턴**\n",
        "6. **일반적인 훈련 루프 패턴**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 임포트\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"PyTorch 버전:\", torch.__version__)\n",
        "print(\"CUDA 사용 가능:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. nn.Module 상속 패턴\n",
        "\n",
        "PyTorch에서 모든 신경망 모델은 `nn.Module`을 상속받아 구현됩니다. 다양한 상속 패턴을 살펴보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 패턴 1: 기본적인 nn.Module 상속\n",
        "class BasicLinearModel(nn.Module):\n",
        "    \"\"\"가장 기본적인 nn.Module 상속 패턴\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(BasicLinearModel, self).__init__()  # 또는 super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 모델 인스턴스 생성 및 테스트\n",
        "basic_model = BasicLinearModel(10, 20, 5)\n",
        "print(\"BasicLinearModel:\", basic_model)\n",
        "\n",
        "# 입력 데이터로 테스트\n",
        "test_input = torch.randn(3, 10)  # batch_size=3, input_size=10\n",
        "output = basic_model(test_input)\n",
        "print(f\"입력 크기: {test_input.shape}\")\n",
        "print(f\"출력 크기: {output.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 패턴 2: 조건부 레이어를 포함한 모델\n",
        "class ConditionalModel(nn.Module):\n",
        "    \"\"\"조건부 레이어 및 드롭아웃을 포함한 패턴\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size, use_dropout=True, dropout_p=0.5):\n",
        "        super(ConditionalModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.use_dropout = use_dropout\n",
        "        if self.use_dropout:\n",
        "            self.dropout = nn.Dropout(dropout_p)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "            \n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "            \n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "conditional_model = ConditionalModel(10, 20, 5, use_dropout=True)\n",
        "print(\"ConditionalModel:\", conditional_model)\n",
        "print(f\"Dropout 사용: {conditional_model.use_dropout}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 패턴 3: ModuleList를 사용한 동적 레이어 구성\n",
        "class DynamicModel(nn.Module):\n",
        "    \"\"\"ModuleList를 사용한 동적 레이어 구성 패턴\"\"\"\n",
        "    def __init__(self, layer_sizes):\n",
        "        super(DynamicModel, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        \n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
        "            \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            # 마지막 레이어가 아니면 활성화 함수 적용\n",
        "            if i < len(self.layers) - 1:\n",
        "                x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "# 동적 아키텍처 정의: [입력 -> 20 -> 15 -> 출력]\n",
        "dynamic_model = DynamicModel([10, 20, 15, 5])\n",
        "print(\"DynamicModel:\", dynamic_model)\n",
        "print(f\"레이어 수: {len(dynamic_model.layers)}\")\n",
        "\n",
        "# 출력 테스트\n",
        "output = dynamic_model(test_input)\n",
        "print(f\"동적 모델 출력 크기: {output.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Dataset 커스텀 패턴\n",
        "\n",
        "PyTorch에서 데이터를 효율적으로 로드하기 위해 `Dataset` 클래스를 상속받아 커스텀 데이터셋을 구현할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 패턴 1: 기본적인 Dataset 상속\n",
        "class SimpleDataset(Dataset):\n",
        "    \"\"\"가장 기본적인 Dataset 상속 패턴\"\"\"\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        target = self.targets[idx]\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "            \n",
        "        return sample, target\n",
        "\n",
        "# 더미 데이터 생성\n",
        "dummy_data = np.random.randn(100, 10)\n",
        "dummy_targets = np.random.randn(100, 1)\n",
        "\n",
        "simple_dataset = SimpleDataset(dummy_data, dummy_targets)\n",
        "print(f\"SimpleDataset 길이: {len(simple_dataset)}\")\n",
        "\n",
        "# 샘플 확인\n",
        "sample_data, sample_target = simple_dataset[0]\n",
        "print(f\"샘플 데이터 타입: {type(sample_data)}, 타겟 타입: {type(sample_target)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 패턴 2: NumPy 배열을 위한 Dataset\n",
        "class NumpyDataset(Dataset):\n",
        "    \"\"\"NumPy 배열을 위한 Dataset 패턴\"\"\"\n",
        "    def __init__(self, data_array, target_array, dtype=torch.float32):\n",
        "        self.data = torch.from_numpy(data_array).type(dtype)\n",
        "        self.targets = torch.from_numpy(target_array).type(dtype)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]\n",
        "\n",
        "numpy_dataset = NumpyDataset(dummy_data, dummy_targets)\n",
        "print(f\"NumpyDataset 길이: {len(numpy_dataset)}\")\n",
        "\n",
        "sample_data, sample_target = numpy_dataset[0]\n",
        "print(f\"샘플 데이터 크기: {sample_data.shape}, 타겟 크기: {sample_target.shape}\")\n",
        "print(f\"데이터 타입: {sample_data.dtype}, 타겟 타입: {sample_target.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 패턴 3: 합성 데이터를 생성하는 Dataset\n",
        "class SyntheticDataset(Dataset):\n",
        "    \"\"\"합성 데이터를 생성하는 Dataset 패턴\"\"\"\n",
        "    def __init__(self, num_samples, input_dim, noise_std=0.1):\n",
        "        self.num_samples = num_samples\n",
        "        self.input_dim = input_dim\n",
        "        self.noise_std = noise_std\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # 간단한 선형 관계 y = 2*x + 1 + noise\n",
        "        x = torch.randn(self.input_dim)\n",
        "        y = 2 * x.sum() + 1 + torch.randn(1) * self.noise_std\n",
        "        return x, y\n",
        "\n",
        "synthetic_dataset = SyntheticDataset(num_samples=100, input_dim=10)\n",
        "print(f\"SyntheticDataset 길이: {len(synthetic_dataset)}\")\n",
        "\n",
        "sample_data, sample_target = synthetic_dataset[0]\n",
        "print(f\"합성 데이터 크기: {sample_data.shape}, 타겟 크기: {sample_target.shape}\")\n",
        "print(f\"샘플 값 - x 평균: {sample_data.mean():.4f}, y: {sample_target.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. DataLoader 사용법 패턴\n",
        "\n",
        "`DataLoader`는 데이터셋을 배치 단위로 효율적으로 로드하는 도구입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 기본 DataLoader 사용\n",
        "dataloader = DataLoader(numpy_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
        "\n",
        "print(f\"DataLoader 배치 크기: {dataloader.batch_size}\")\n",
        "print(f\"DataLoader 데이터셋 크기: {len(dataloader.dataset)}\")\n",
        "print(f\"배치 수: {len(dataloader)}\")\n",
        "\n",
        "# DataLoader 반복 패턴\n",
        "print(\"\\nDataLoader 반복 패턴:\")\n",
        "for batch_idx, (data, target) in enumerate(dataloader):\n",
        "    print(f\"배치 {batch_idx}: 데이터 크기 {data.shape}, 타겟 크기 {target.shape}\")\n",
        "    if batch_idx >= 2:  # 처음 3개 배치만 출력\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 커스텀 collate_fn 패턴\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"커스텀 배치 처리 함수\"\"\"\n",
        "    data, targets = zip(*batch)\n",
        "    data = torch.stack(data)\n",
        "    targets = torch.stack(targets)\n",
        "    \n",
        "    # 예시: 데이터 정규화\n",
        "    data = (data - data.mean()) / (data.std() + 1e-8)\n",
        "    \n",
        "    return data, targets\n",
        "\n",
        "custom_dataloader = DataLoader(\n",
        "    numpy_dataset, \n",
        "    batch_size=8, \n",
        "    shuffle=True, \n",
        "    collate_fn=custom_collate_fn,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(\"커스텀 collate_fn을 사용한 DataLoader:\")\n",
        "for batch_idx, (data, target) in enumerate(custom_dataloader):\n",
        "    print(f\"배치 {batch_idx}: 데이터 평균 {data.mean():.4f}, 데이터 표준편차 {data.std():.4f}\")\n",
        "    if batch_idx >= 1:  # 처음 2개 배치만 출력\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. train() / eval() 모드와 no_grad() 패턴\n",
        "\n",
        "모델의 훈련 모드와 평가 모드를 적절히 전환하고, 기울기 계산을 제어하는 패턴입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train() / eval() 모드 차이 확인\n",
        "model_with_dropout = ConditionalModel(10, 20, 5, use_dropout=True, dropout_p=0.5)\n",
        "\n",
        "print(\"=== train() 모드 ===\")\n",
        "model_with_dropout.train()  # 훈련 모드로 설정\n",
        "print(f\"모델 훈련 모드: {model_with_dropout.training}\")\n",
        "\n",
        "test_input = torch.randn(5, 10)\n",
        "output1 = model_with_dropout(test_input)\n",
        "output2 = model_with_dropout(test_input)  # 같은 입력이지만 드롭아웃으로 인해 다른 출력\n",
        "\n",
        "print(f\"훈련 모드 - 출력1 평균: {output1.mean():.4f}\")\n",
        "print(f\"훈련 모드 - 출력2 평균: {output2.mean():.4f}\")\n",
        "print(f\"드롭아웃으로 인해 출력이 다름: {not torch.allclose(output1, output2)}\")\n",
        "\n",
        "print(\"\\n=== eval() 모드 ===\")\n",
        "model_with_dropout.eval()  # 평가 모드로 설정\n",
        "print(f\"모델 훈련 모드: {model_with_dropout.training}\")\n",
        "\n",
        "output3 = model_with_dropout(test_input)\n",
        "output4 = model_with_dropout(test_input)  # 같은 입력, 드롭아웃 비활성화로 같은 출력\n",
        "\n",
        "print(f\"평가 모드 - 출력3 평균: {output3.mean():.4f}\")\n",
        "print(f\"평가 모드 - 출력4 평균: {output4.mean():.4f}\")\n",
        "print(f\"출력이 동일함 (드롭아웃 없음): {torch.allclose(output3, output4)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.no_grad() 패턴\n",
        "print(\"=== torch.no_grad() 패턴 ===\")\n",
        "model_with_dropout.train()\n",
        "input_with_grad = torch.randn(5, 10, requires_grad=True)\n",
        "\n",
        "# 기울기 추적 O\n",
        "output_with_grad = model_with_dropout(input_with_grad)\n",
        "print(f\"기울기 추적 O - 출력 requires_grad: {output_with_grad.requires_grad}\")\n",
        "print(f\"입력 grad_fn: {input_with_grad.grad_fn}\")\n",
        "print(f\"출력 grad_fn: {output_with_grad.grad_fn}\")\n",
        "\n",
        "# 기울기 추적 X\n",
        "with torch.no_grad():\n",
        "    output_no_grad = model_with_dropout(input_with_grad)\n",
        "    print(f\"기울기 추적 X - 출력 requires_grad: {output_no_grad.requires_grad}\")\n",
        "    print(f\"기울기 추적 X - 출력 grad_fn: {output_no_grad.grad_fn}\")\n",
        "\n",
        "print(f\"\\n메모리 효율성: no_grad() 사용 시 계산 그래프가 생성되지 않아 메모리 절약\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. 모델 저장/로드 패턴\n",
        "\n",
        "모델을 저장하고 로드하는 다양한 방법을 살펴보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 훈련된 모델 생성 (간단한 훈련 시뮬레이션)\n",
        "model_to_save = BasicLinearModel(10, 20, 5)\n",
        "optimizer = optim.Adam(model_to_save.parameters(), lr=0.001)\n",
        "\n",
        "# 간단한 훈련 루프 시뮬레이션\n",
        "model_to_save.train()\n",
        "for epoch in range(3):\n",
        "    optimizer.zero_grad()\n",
        "    dummy_input = torch.randn(32, 10)\n",
        "    dummy_target = torch.randn(32, 5)\n",
        "    output = model_to_save(dummy_input)\n",
        "    loss = F.mse_loss(output, dummy_target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"에폭 {epoch+1}, 손실: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 패턴 1: 전체 모델 저장/로드\n",
        "print(\"=== 패턴 1: 전체 모델 저장/로드 ===\")\n",
        "model_path = \"temp_full_model.pth\"\n",
        "torch.save(model_to_save, model_path)\n",
        "print(f\"전체 모델이 {model_path}에 저장됨\")\n",
        "\n",
        "loaded_model = torch.load(model_path)\n",
        "print(f\"모델 로드 성공\")\n",
        "print(f\"원본 모델 타입: {type(model_to_save)}\")\n",
        "print(f\"로드된 모델 타입: {type(loaded_model)}\")\n",
        "\n",
        "# 정리\n",
        "if os.path.exists(model_path):\n",
        "    os.remove(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 패턴 2: state_dict 저장/로드 (권장 방법)\n",
        "print(\"=== 패턴 2: state_dict 저장/로드 (권장) ===\")\n",
        "state_dict_path = \"temp_state_dict.pth\"\n",
        "\n",
        "# 모델 상태 저장\n",
        "torch.save(model_to_save.state_dict(), state_dict_path)\n",
        "print(f\"모델 state_dict가 {state_dict_path}에 저장됨\")\n",
        "\n",
        "# 새 모델 인스턴스 생성 후 state_dict 로드\n",
        "new_model = BasicLinearModel(10, 20, 5)\n",
        "new_model.load_state_dict(torch.load(state_dict_path))\n",
        "print(f\"State dict가 새 모델에 로드됨\")\n",
        "\n",
        "# 모델 파라미터 비교\n",
        "original_params = list(model_to_save.parameters())\n",
        "loaded_params = list(new_model.parameters())\n",
        "\n",
        "params_equal = all(torch.allclose(p1, p2) for p1, p2 in zip(original_params, loaded_params))\n",
        "print(f\"파라미터 동일성: {params_equal}\")\n",
        "\n",
        "# 정리\n",
        "if os.path.exists(state_dict_path):\n",
        "    os.remove(state_dict_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 패턴 3: 체크포인트 저장/로드 (모델 + 옵티마이저 + 기타 정보)\n",
        "print(\"=== 패턴 3: 체크포인트 저장/로드 ===\")\n",
        "checkpoint_path = \"temp_checkpoint.pth\"\n",
        "\n",
        "# 체크포인트 저장\n",
        "checkpoint = {\n",
        "    'epoch': 3,\n",
        "    'model_state_dict': model_to_save.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss.item(),\n",
        "    'model_config': {\n",
        "        'input_size': 10,\n",
        "        'hidden_size': 20,\n",
        "        'output_size': 5\n",
        "    }\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, checkpoint_path)\n",
        "print(f\"체크포인트가 {checkpoint_path}에 저장됨\")\n",
        "\n",
        "# 체크포인트 로드\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "restored_model = BasicLinearModel(\n",
        "    checkpoint['model_config']['input_size'],\n",
        "    checkpoint['model_config']['hidden_size'],\n",
        "    checkpoint['model_config']['output_size']\n",
        ")\n",
        "restored_optimizer = optim.Adam(restored_model.parameters(), lr=0.001)\n",
        "\n",
        "restored_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "restored_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "start_epoch = checkpoint['epoch']\n",
        "last_loss = checkpoint['loss']\n",
        "\n",
        "print(f\"체크포인트 로드됨 - 에폭: {start_epoch}, 마지막 손실: {last_loss:.4f}\")\n",
        "\n",
        "# 정리\n",
        "if os.path.exists(checkpoint_path):\n",
        "    os.remove(checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 패턴 4: 장치 호환성 (GPU/CPU)\n",
        "print(\"=== 패턴 4: 장치 호환성 ===\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"현재 장치: {device}\")\n",
        "\n",
        "# 장치에 맞는 로드 시뮬레이션\n",
        "if device.type == 'cuda':\n",
        "    print(\"GPU에서 로드하는 경우\")\n",
        "    # GPU에서 로드: loaded_checkpoint = torch.load(checkpoint_path)\n",
        "else:\n",
        "    print(\"CPU에서 로드하는 경우 (GPU에서 저장된 모델도 CPU로 로드 가능)\")\n",
        "    # CPU에서 로드: loaded_checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "print(\"map_location 매개변수를 사용하여 장치 간 호환성 보장\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. 일반적인 훈련 루프 패턴\n",
        "\n",
        "효율적이고 재사용 가능한 훈련 및 평가 함수를 구현해보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 표준 훈련 및 평가 함수 정의\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"한 에폭 훈련을 수행하는 함수\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 순전파\n",
        "        output = model(data)\n",
        "        \n",
        "        # 손실 계산\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "        \n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    \"\"\"모델 평가를 수행하는 함수\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "    \n",
        "    avg_loss = total_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "print(\"훈련 및 평가 함수 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 전체 훈련 루프 실행 예시\n",
        "print(\"=== 훈련 루프 패턴 실행 ===\")\n",
        "device = torch.device('cpu')  # 예시를 위해 CPU 사용\n",
        "model = BasicLinearModel(10, 20, 1).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 데이터셋 및 데이터로더 생성\n",
        "train_dataset = SyntheticDataset(num_samples=1000, input_dim=10)\n",
        "val_dataset = SyntheticDataset(num_samples=200, input_dim=10)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 훈련 실행\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss = evaluate_model(model, val_loader, criterion, device)\n",
        "    \n",
        "    print(f\"에폭 {epoch+1}/{num_epochs} - 훈련 손실: {train_loss:.4f}, 검증 손실: {val_loss:.4f}\")\n",
        "\n",
        "print(\"\\n훈련 완료!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 요약\n",
        "\n",
        "이 노트북에서 다룬 PyTorch 핵심 구현 패턴:\n",
        "\n",
        "1. **nn.Module 상속**: 기본, 조건부, 동적 모델 구현\n",
        "2. **Dataset 커스텀**: 다양한 데이터 로딩 패턴\n",
        "3. **DataLoader**: 효율적인 배치 처리 및 커스텀 collate_fn\n",
        "4. **train()/eval() 모드**: 모델 상태 제어 및 기울기 관리\n",
        "5. **모델 저장/로드**: 다양한 저장 방식과 장치 호환성\n",
        "6. **훈련 루프**: 재사용 가능한 훈련 및 평가 함수\n",
        "\n",
        "이러한 패턴들을 숙지하면 효율적이고 유지보수가 쉬운 PyTorch 코드를 작성할 수 있습니다.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
