"""
ResNet ì•„í‚¤í…ì²˜ ë¶„ì„ ë° ì‹œê°í™”
===============================

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ResNetì˜ ì•„í‚¤í…ì²˜ë¥¼ ë¶„ì„í•˜ê³  Skip Connectionì˜ íš¨ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
ì‹¤ì œ í•™ìŠµì€ í•˜ì§€ ì•Šê³  êµ¬ì¡° ë¶„ì„ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ì£¼ìš” ë¶„ì„ ë‚´ìš©:
1. ResNetê³¼ ì¼ë°˜ CNNì˜ êµ¬ì¡° ë¹„êµ
2. Skip Connectionì˜ gradient flow ê°œì„  íš¨ê³¼
3. ë‹¤ì–‘í•œ ResNet ë³€í˜•ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ
4. Residual Blockì˜ êµ¬ì¡° ë¶„ì„
"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from abc import ABC, abstractmethod
from typing import List, Tuple

print("=== ResNet ì•„í‚¤í…ì²˜ ë¶„ì„ ===\n")


# ResNet í´ë˜ìŠ¤ë“¤ ì •ì˜ (ë¶„ì„ìš©)
class ResidualBlock(nn.Module, ABC):
    """ì”ì°¨ ë¸”ë¡ì˜ ì¶”ìƒ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ ì •ì˜ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass


class BasicBlock(ResidualBlock):
    """ê¸°ë³¸ ì”ì°¨ ë¸”ë¡ êµ¬í˜„"""
    
    expansion: int = 1  # ì¶œë ¥ ì±„ë„ í™•ì¥ ë¹„ìœ¨
    
    def __init__(self, in_planes: int, planes: int, stride: int = 1) -> None:
        super().__init__()
        
        # ì£¼ ê²½ë¡œ (main path)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, 
                              padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, 
                              padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        
        # ìˆì»· ê²½ë¡œ (shortcut path) - ì°¨ì› ë§ì¶”ê¸°ìš©
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, 
                         stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion * planes)
            )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ: ì”ì°¨ ì—°ê²° ìˆ˜í–‰"""
        # ì£¼ ê²½ë¡œ
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        # ì”ì°¨ ì—°ê²°: F(x) + x
        out += self.shortcut(x)
        out = torch.relu(out)
        
        return out


class Bottleneck(ResidualBlock):
    """ë³‘ëª© ì”ì°¨ ë¸”ë¡ êµ¬í˜„ (ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ìš©)"""
    
    expansion: int = 4  # ì¶œë ¥ ì±„ë„ í™•ì¥ ë¹„ìœ¨
    
    def __init__(self, in_planes: int, planes: int, stride: int = 1) -> None:
        super().__init__()
        
        # ì£¼ ê²½ë¡œ: 1x1 -> 3x3 -> 1x1 êµ¬ì¡°
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, 
                              padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion * planes)
        
        # ìˆì»· ê²½ë¡œ
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, 
                         stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion * planes)
            )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ: ë³‘ëª© êµ¬ì¡°ì˜ ì”ì°¨ ì—°ê²° ìˆ˜í–‰"""
        # ì£¼ ê²½ë¡œ
        out = torch.relu(self.bn1(self.conv1(x)))
        out = torch.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        
        # ì”ì°¨ ì—°ê²°
        out += self.shortcut(x)
        out = torch.relu(out)
        
        return out


class ResNet(nn.Module):
    """ResNet ëª¨ë¸ êµ¬í˜„"""
    
    def __init__(self, block_type: type[ResidualBlock], num_blocks: List[int], 
                 num_classes: int = 10) -> None:
        super().__init__()
        
        self.in_planes = 64
        
        # ì´ˆê¸° ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        
        # ì”ì°¨ ë ˆì´ì–´ë“¤
        self.layer1 = self._make_layer(block_type, 64, num_blocks[0], 1)
        self.layer2 = self._make_layer(block_type, 128, num_blocks[1], 2)
        self.layer3 = self._make_layer(block_type, 256, num_blocks[2], 2)
        self.layer4 = self._make_layer(block_type, 512, num_blocks[3], 2)
        
        # ë¶„ë¥˜ê¸°
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block_type.expansion, num_classes)
    
    def _make_layer(self, block_type: type[ResidualBlock], planes: int, 
                   num_blocks: int, stride: int) -> nn.Sequential:
        """ResNet ë ˆì´ì–´ ìƒì„± (ë‚´ë¶€ ë©”ì„œë“œ)"""
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        
        for stride in strides:
            layers.append(block_type(self.in_planes, planes, stride))
            self.in_planes = planes * block_type.expansion
        
        return nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ"""
        # ì´ˆê¸° ì»¨ë³¼ë£¨ì…˜
        out = torch.relu(self.bn1(self.conv1(x)))
        
        # ì”ì°¨ ë ˆì´ì–´ë“¤
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        
        # ë¶„ë¥˜
        out = self.avg_pool(out)
        out = torch.flatten(out, 1)
        out = self.fc(out)
        
        return out


class ResNetFactory:
    """ë‹¤ì–‘í•œ ResNet ë³€í˜•ì„ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def resnet18(num_classes: int = 10) -> ResNet:
        """ResNet-18 ëª¨ë¸ ìƒì„±"""
        return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)
    
    @staticmethod
    def resnet34(num_classes: int = 10) -> ResNet:
        """ResNet-34 ëª¨ë¸ ìƒì„±"""
        return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)
    
    @staticmethod
    def resnet50(num_classes: int = 10) -> ResNet:
        """ResNet-50 ëª¨ë¸ ìƒì„±"""
        return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)
    
    @staticmethod
    def resnet101(num_classes: int = 10) -> ResNet:
        """ResNet-101 ëª¨ë¸ ìƒì„±"""
        return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)


class SimpleConvNet(nn.Module):
    """ë¹„êµë¥¼ ìœ„í•œ ì¼ë°˜ì ì¸ CNN êµ¬ì¡°"""
    
    def __init__(self, num_classes: int = 10):
        super().__init__()
        self.features = nn.Sequential(
            # Layer 1
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            # Layer 2
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            # Layer 3
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            
            # Layer 4
            nn.Conv2d(256, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        self.classifier = nn.Linear(512, num_classes)
    
    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x


def count_parameters(model: nn.Module) -> int:
    """ëª¨ë¸ì˜ ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def analyze_resnet_variants():
    """ë‹¤ì–‘í•œ ResNet ë³€í˜•ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ ë¶„ì„"""
    print("1. ResNet ë³€í˜•ë³„ íŒŒë¼ë¯¸í„° ìˆ˜ ë¶„ì„")
    print("=" * 50)
    
    models = {
        'ResNet-18': ResNetFactory.resnet18(),
        'ResNet-34': ResNetFactory.resnet34(),
        'ResNet-50': ResNetFactory.resnet50(),
        'ResNet-101': ResNetFactory.resnet101(),
        'Simple CNN': SimpleConvNet()
    }
    
    results = []
    for name, model in models.items():
        params = count_parameters(model)
        results.append((name, params))
        print(f"{name:12s}: {params:,} íŒŒë¼ë¯¸í„°")
    
    print(f"\nğŸ’¡ ê´€ì°°:")
    print(f"   - ResNet-50ë¶€í„°ëŠ” Bottleneck ë¸”ë¡ì„ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì„± ì¦ëŒ€")
    print(f"   - Skip Connectionìœ¼ë¡œ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë„ ì•ˆì •ì  í›ˆë ¨ ê°€ëŠ¥")
    print(f"   - ì¼ë°˜ CNN ëŒ€ë¹„ ResNetì€ ë” ê¹Šìœ¼ë©´ì„œë„ íš¨ìœ¨ì \n")
    
    return results


def visualize_skip_connection():
    """Skip Connectionì˜ íš¨ê³¼ ì‹œê°í™”"""
    print("2. Skip Connectionì˜ Gradient Flow íš¨ê³¼")
    print("=" * 50)
    
    # ì‹œë®¬ë ˆì´ì…˜ëœ gradient ê°’ë“¤
    layers = list(range(1, 19))  # 18ê°œ ë ˆì´ì–´
    
    # ì¼ë°˜ CNNì˜ gradient (vanishing í˜„ìƒ)
    normal_gradients = [1.0 * (0.8 ** (18 - i)) for i in layers]
    
    # ResNetì˜ gradient (skip connectionìœ¼ë¡œ ì•ˆì •í™”)
    resnet_gradients = [max(0.3, 1.0 * (0.95 ** (18 - i))) for i in layers]
    
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    plt.plot(layers, normal_gradients, 'r-o', label='ì¼ë°˜ CNN', linewidth=2)
    plt.plot(layers, resnet_gradients, 'b-s', label='ResNet', linewidth=2)
    plt.xlabel('ë ˆì´ì–´ ê¹Šì´')
    plt.ylabel('Gradient í¬ê¸°')
    plt.title('Gradient Flow ë¹„êµ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    
    plt.subplot(1, 2, 2)
    # Skip connection êµ¬ì¡° ì‹œê°í™”
    x = np.linspace(0, 4, 100)
    
    # Main path (ë¹„ì„ í˜•)
    main_path = np.sin(x) * 0.5 + 0.5
    
    # Skip connection (ì§ì„ )
    skip_path = np.linspace(0.2, 0.8, 100)
    
    # í•©ì„± ê²½ë¡œ
    combined = main_path + skip_path
    
    plt.plot(x, main_path, 'r--', label='Main Path F(x)', linewidth=2)
    plt.plot(x, skip_path, 'g-', label='Skip Connection x', linewidth=2)
    plt.plot(x, combined, 'b-', label='Output F(x) + x', linewidth=3)
    plt.xlabel('ì…ë ¥')
    plt.ylabel('ì¶œë ¥')
    plt.title('Skip Connection êµ¬ì¡°')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('explain/resnet_analysis.png', dpi=300, bbox_inches='tight')
    
    print("ê·¸ë˜í”„ê°€ 'explain/resnet_analysis.png'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"\nğŸ’¡ Skip Connectionì˜ íš¨ê³¼:")
    print(f"   - Gradient vanishing ë¬¸ì œ ì™„í™”")
    print(f"   - Identity mappingì„ í†µí•œ ì•ˆì •ì ì¸ í•™ìŠµ")
    print(f"   - ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œë„ íš¨ê³¼ì ì¸ ì—­ì „íŒŒ\n")


def analyze_block_structure():
    """Residual Block êµ¬ì¡° ë¶„ì„"""
    print("3. Residual Block êµ¬ì¡° ë¶„ì„")
    print("=" * 50)
    
    # BasicBlock ë¶„ì„
    basic_block = BasicBlock(64, 64)
    basic_params = count_parameters(basic_block)
    
    # Bottleneck ë¶„ì„
    bottleneck = Bottleneck(256, 64)
    bottleneck_params = count_parameters(bottleneck)
    
    print(f"BasicBlock (ResNet-18/34):")
    print(f"  - êµ¬ì¡°: 3x3 conv â†’ BN â†’ ReLU â†’ 3x3 conv â†’ BN â†’ (+skip) â†’ ReLU")
    print(f"  - íŒŒë¼ë¯¸í„° ìˆ˜: {basic_params:,}")
    print(f"  - í™•ì¥ ë¹„ìœ¨: {BasicBlock.expansion}x")
    print(f"  - ì‚¬ìš© ëª¨ë¸: ResNet-18, ResNet-34")
    
    print(f"\nBottleneck (ResNet-50+):")
    print(f"  - êµ¬ì¡°: 1x1 conv â†’ BN â†’ ReLU â†’ 3x3 conv â†’ BN â†’ ReLU â†’ 1x1 conv â†’ BN â†’ (+skip) â†’ ReLU")
    print(f"  - íŒŒë¼ë¯¸í„° ìˆ˜: {bottleneck_params:,}")
    print(f"  - í™•ì¥ ë¹„ìœ¨: {Bottleneck.expansion}x")
    print(f"  - ì‚¬ìš© ëª¨ë¸: ResNet-50, ResNet-101, ResNet-152")
    
    print(f"\nğŸ’¡ Bottleneckì˜ ì¥ì :")
    print(f"   - 1x1 convë¡œ ì°¨ì› ì¶•ì†Œ í›„ 3x3 conv ì ìš©í•˜ì—¬ ê³„ì‚°ëŸ‰ ê°ì†Œ")
    print(f"   - ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ì„± ê°€ëŠ¥")
    print(f"   - í‘œí˜„ë ¥ì€ ìœ ì§€í•˜ë©´ì„œ íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” ì ˆì•½\n")


def demonstrate_forward_pass():
    """ìˆœì „íŒŒ ê³¼ì • ì‹œì—° (ì‹¤ì œ ê³„ì‚° ì—†ì´ êµ¬ì¡°ë§Œ)"""
    print("4. ResNet ìˆœì „íŒŒ ê³¼ì • ì‹œì—°")
    print("=" * 50)
    
    model = ResNetFactory.resnet18()
    model.eval()
    
    # ì˜ˆì‹œ ì…ë ¥ í¬ê¸° (ì‹¤ì œ ë°ì´í„°ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    input_shape = (1, 3, 32, 32)
    
    print(f"ì…ë ¥ í¬ê¸°: {input_shape}")
    print(f"ì´ˆê¸° conv í›„: (1, 64, 32, 32)")
    print(f"Layer 1 í›„: (1, 64, 32, 32)")
    print(f"Layer 2 í›„: (1, 128, 16, 16)")
    print(f"Layer 3 í›„: (1, 256, 8, 8)")
    print(f"Layer 4 í›„: (1, 512, 4, 4)")
    print(f"Global Avg Pool í›„: (1, 512, 1, 1)")
    print(f"ìµœì¢… ì¶œë ¥: (1, 10)")
    
    print(f"\nğŸ’¡ íŠ¹ì§•:")
    print(f"   - ê° layerë§ˆë‹¤ ì±„ë„ ìˆ˜ëŠ” 2ë°°ì”© ì¦ê°€")
    print(f"   - ê³µê°„ í•´ìƒë„ëŠ” ì ˆë°˜ì”© ê°ì†Œ")
    print(f"   - Skip connectionìœ¼ë¡œ gradient flow ì•ˆì •í™”")
    print(f"   - Global Average Poolingìœ¼ë¡œ ìœ„ì¹˜ ë¶ˆë³€ì„± í™•ë³´\n")


def compare_computational_complexity():
    """ê³„ì‚° ë³µì¡ë„ ë¹„êµ"""
    print("5. ê³„ì‚° ë³µì¡ë„ ë¹„êµ")
    print("=" * 50)
    
    models = {
        'SimpleConv': SimpleConvNet(),
        'ResNet-18': ResNetFactory.resnet18(),
        'ResNet-50': ResNetFactory.resnet50()
    }
    
    for name, model in models.items():
        params = count_parameters(model)
        
        print(f"{name}:")
        print(f"  - íŒŒë¼ë¯¸í„° ìˆ˜: {params:,}")
        print(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~{params * 4 / (1024**2):.1f} MB")
        
    print(f"\nğŸ’¡ íš¨ìœ¨ì„±:")
    print(f"   - ResNetì€ ê¹Šì´ ëŒ€ë¹„ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ")
    print(f"   - Skip connectionìœ¼ë¡œ í›ˆë ¨ ì•ˆì •ì„± í™•ë³´")
    print(f"   - Bottleneck êµ¬ì¡°ë¡œ ê³„ì‚°ëŸ‰ ìµœì í™”\n")


if __name__ == "__main__":
    # ë¶„ì„ ì‹¤í–‰ (í•™ìŠµì€ í•˜ì§€ ì•ŠìŒ)
    analyze_resnet_variants()
    analyze_block_structure()
    demonstrate_forward_pass()
    compare_computational_complexity()
    visualize_skip_connection()
    
    print("=== ë¶„ì„ ì™„ë£Œ ===")
    print("ResNetì˜ í•µì‹¬ ì•„ì´ë””ì–´ì™€ êµ¬ì¡°ì  ì¥ì ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤!")
    print("ì‹¤ì œ í•™ìŠµì„ í•˜ë ¤ë©´ 'python learn/06_resnet.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.") 